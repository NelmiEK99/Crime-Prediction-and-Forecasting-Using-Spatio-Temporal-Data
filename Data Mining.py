# -*- coding: utf-8 -*-
"""DM_Group_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XDsXJsC5gzF1WJhrVbCYiDgOLan9hUY1

#Title - Title: Crime Prediction and Forecasting Using Spatio-Temporal Data

**Description**:
In this notebook, we explore the fascinating realm of crime prediction and forecasting using spatio-temporal data. Leveraging a comprehensive crime dataset, we delve into the core question of whether a person will be arrested or not. Our journey takes us through various machine learning algorithms, offering insights into the predictive power of decision trees and random forests.

**The key highlights of our exploration include:**

**Utilizing Decision Trees:** We harness the potential of decision tree algorithms to create predictive models that aid in determining arrest outcomes. By optimizing the model's hyperparameters, we strive to enhance its accuracy and reliability.

**Harnessing Random Forests:** We extend our predictive capabilities by employing the Random Forest algorithm, a powerful ensemble learning technique. This approach allows us to leverage the collective intelligence of multiple decision trees to make more robust predictions.

**Spatial Insights with K-Means:** To unearth hidden spatial patterns within the data, we apply the K-Means clustering algorithm. This method helps us identify clusters of similar crime occurrences and provides valuable spatial insights.

**Proximity Analysis with PAM:** Utilizing Partitioning Around Medoids (PAM) clustering, we further explore patterns in the data. PAM analysis focuses on identifying central points (medoids) within crime clusters, shedding light on the proximity dynamics of criminal activities.

**Hierarchical Clustering:** We employ hierarchical clustering techniques to uncover the hierarchical structure within the data. This method helps us visualize crime groupings and their relationships in a hierarchical manner.

#Importing Libraries and Dataset Overview
"""

pip install scikit-learn-extra

import numpy as np
import timeit
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn_extra.cluster import KMedoids
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

df = pd.read_csv('sample_data/Crimes2022.csv')
df.head()

print('Dataset length before drop null values')
len(df)

df.info()

df.isnull().sum()

#Unique crime types
df['Primary Type'].unique()

"""# Data Visualization"""

# list of categorical variables to plot
cat_vars = ['Primary Type','Domestic']

# create figure with subplots
fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))
axs = axs.flatten()

# create barplot for each categorical variable
for i, var in enumerate(cat_vars):
    sns.countplot(x=var, hue='Arrest', data=df, ax=axs[i])
    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=90)

# adjust spacing between subplots
fig.tight_layout()

# show plot
plt.show()

"""# Data Preprocessing"""

df.dropna(inplace=True)

df.isnull().sum()

print('Dataset length after removing null values')
len(df)

drop_colmns = ['Case Number', 'X Coordinate', 'Y Coordinate', 'ID', 'Updated On', 'FBI Code', 'Ward', 'IUCR', 'Ward', 'Community Area', 'Block', 'Location']
df_dropped = df.drop(columns=drop_colmns)

df_dropped.dtypes

#Convert date to match format for Pandas and create an index using the 'Date' feature

df_dropped['Date'] = pd.to_datetime(df_dropped['Date'], format='%m/%d/%Y %I:%M:%S %p')
df_dropped.index = pd.DatetimeIndex(df_dropped['Date'])

#Add the features of 'Month' and 'Weekday' using the date index

df_dropped['Month'] = df_dropped.index.month.astype(int)
df_dropped['Weekday'] = df_dropped.index.weekday.astype(int)
df_dropped.head(10)

#Drop the 'Date' feature
df_dropped.drop(columns=['Date'], axis=1, inplace=True, errors='ignore')
df_dropped2 = df_dropped
df_dropped.head(10)

#Display the value counts for the feature of 'Year'

value_counts = df_dropped['Year'].value_counts()
print(value_counts)

#Drop the 'Year' feature value of 2017

df_dropped.drop(df_dropped[df_dropped['Year'] == 2017].index, inplace=True, errors='ignore')
value_counts = df_dropped['Year'].value_counts()
print(value_counts)

#Display the value counts for feature of 'Primary Type'

value_counts = df_dropped['Primary Type'].value_counts()
print(value_counts)

#Drop the 'Primary Type' categories that have a value count less than 1000"""

value_counts = df_dropped['Primary Type'].value_counts()
remove_values = value_counts[value_counts < 1000].index
crimes_final = df_dropped[~df_dropped['Primary Type'].isin(remove_values)]

domestic = crimes_final[crimes_final['Domestic'] == True]

plt.figure(figsize=(8, 8))
domestic.groupby([domestic['Primary Type']]).size().sort_values(ascending=False)[:15].plot(kind='barh')
plt.xlabel('Number of Crimes')
plt.ylabel('Primary Type')
plt.show()

# Encoding all remaining columns:
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
Primary_Type_array = le.fit_transform(crimes_final['Primary Type'])
Description_array = le.fit_transform(crimes_final['Description'])
Location_Description_array = le.fit_transform(crimes_final['Location Description'])
Arrest_array = le.fit_transform(crimes_final['Arrest'])
Domestic_array = le.fit_transform(crimes_final['Domestic'])
Year_array = le.fit_transform(crimes_final['Year'])

crimes_final = crimes_final.drop(columns=['Primary Type', 'Description', 'Location Description', 'Arrest', 'Domestic'])
crimes_final = crimes_final.drop(columns=['Year'])
crimes_final['Primary Type'] = Primary_Type_array
crimes_final['Description'] = Description_array
crimes_final['Location Description'] = Location_Description_array
crimes_final['Arrest'] = Arrest_array
crimes_final['Domestic'] = Domestic_array
crimes_final['Year'] = Year_array

crimes_final

crimes_final.head()

"""# Classification Model Development

## Decision Tree
"""



"""In order to classify using decision tree, we need to drop unnecessary columns in the preprocessed dataset."""

drop_colmns = ['Beat', 'Latitude', 'Longitude', 'Description', 'Year', 'District', 'Description']
df_dropped = crimes_final.drop(columns=drop_colmns)

df_dropped.head()

"""Here we can see the class inbalace of our dataset.
1.   Not Arrest (False) - 0
2.   Arrest (True) - 1
"""

import seaborn as sns
sns.countplot(x='Arrest', data=crimes_final, palette='Set1')
crimes_final['Arrest'].value_counts()

# list of categorical variables to plot
cat_vars = ['Month',	'Weekday',	'Primary Type',	'Domestic']

# create figure with subplots
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))
axs = axs.flatten()

# create barplot for each categorical variable
for i, var in enumerate(cat_vars):
    sns.countplot(x=var, hue='Arrest', data=crimes_final, ax=axs[i])
    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=90)

# adjust spacing between subplots
fig.tight_layout()

# show plot
plt.show()

X = df_dropped.drop('Arrest', axis=1)
y = df_dropped['Arrest']

#test size 30% and train size 70%
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=0)

"""Now we need to identify the best hyperparameters for our decision tree model based on the provided hyperparameter ranges."""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
dtree = DecisionTreeClassifier()
param_grid = {
    'max_depth': [3, 4, 5, 6, 7, 8],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3, 4]
}

# Perform a grid search with cross-validation to find the best hyperparameters
grid_search = GridSearchCV(dtree, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print(grid_search.best_params_)

dtree = DecisionTreeClassifier(random_state=0, max_depth=7, min_samples_leaf=2, min_samples_split=3)
dtree.fit(X_train, y_train)

y_pred = dtree.predict(X_test)
print("Accuracy Score :", round(accuracy_score(y_test, y_pred)*100 ,2), "%")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, jaccard_score, log_loss
print('F-1 Score : ',(f1_score(y_test, y_pred, average='micro')))
print('Precision Score : ',(precision_score(y_test, y_pred, average='micro')))
print('Recall Score : ',(recall_score(y_test, y_pred, average='micro')))
print('Jaccard Score : ',(jaccard_score(y_test, y_pred, average='micro')))

import seaborn as sns
imp_df = pd.DataFrame({
    "Feature Name": X_train.columns,
    "Importance": dtree.feature_importances_
})
fi = imp_df.sort_values(by="Importance", ascending=False)

fi2 = fi.head(10)
plt.figure(figsize=(10,8))
sns.barplot(data=fi2, x='Importance', y='Feature Name')
plt.title('Top 5 Feature Importance Each Attributes (Decision Tree)', fontsize=18)
plt.xlabel ('Importance', fontsize=16)
plt.ylabel ('Feature Name', fontsize=16)
plt.show()

from sklearn.tree import plot_tree

fig = plt.figure(figsize=(100,20))
a = plot_tree(dtree, feature_names=df_dropped.columns, fontsize=12, filled=True,
              class_names=['Arrest', 'Not Arrest'])

"""Save decision tree as a PDF file."""

from sklearn.tree import export_graphviz
import graphviz

feature_names = ['Month', 'Weekday', 'Primary Type', 'Location Description', 'Domestic']

# Export the decision tree to a Graphviz DOT format
dot_data = export_graphviz(dtree, out_file=None, feature_names=feature_names, class_names=['Arrest', 'Not Arrest'], filled=True, rounded=True)

# Create a Graphviz object from the DOT data
graph = graphviz.Source(dot_data)

# Save the tree as a PDF file
graph.render("decision_tree")

"""## Random Forest"""

X_RF = crimes_final.drop(['Arrest'],axis=1).values
y_RF = crimes_final['Arrest'].values

X_train, X_test, y_train, y_test = train_test_split(X_RF, y_RF, test_size=0.25, random_state=42)
X_train.shape, X_test.shape

#Model training with imbalance data

classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

print("Accuracy of Random Forest : ",accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
Arrest = crimes_final['Arrest'].unique()
print(classification_report(y_test,y_pred, target_names=['0','1']))

# # Using BalancedRandomForestClassifier

# brf_classifier = BalancedRandomForestClassifier(n_estimators=100, random_state=42)
# brf_classifier.fit(X_train, y_train)

# # predict the mode
# y_pred = brf_classifier.predict(X_test)

# # performance evaluatio metrics
# print(classification_report(y_pred, y_test))

X_tr, X_validation, y_tr, y_validation = train_test_split(X_mine, y_mine, test_size=0.10, random_state=42)

#Hyper parameter tuning step 1
param_grid_random = {
    'n_estimators': [200, 300],
    'min_samples_leaf': [1, 2, 4],
    "min_samples_split": [3, 7, 11],
    "bootstrap": [True, False],
    "criterion": ["gini", "entropy"],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 50],
    'max_leaf_nodes': [10, 20, 50]
}

sampled_X_train, sampled_y_train = X_validation, y_validation
random_search = RandomizedSearchCV(RandomForestClassifier(), param_grid_random, cv=5)
random_search.fit(sampled_X_train, sampled_y_train)
print(random_search.best_estimator_)

#Hyper parameter tuning step 2
param_grid_random = {
    'n_estimators': [450, 550],
    'min_samples_leaf': [2],
    "min_samples_split": [11, 13],
    'max_depth': [45, 50, 55],
    "bootstrap": [False],
    "criterion": ["entropy"],
    'max_features': ['log2'],
}

# Create the Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)

sampled_X_train, sampled_y_train = X_validation, y_validation

# Perform grid search with cross-validation
grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')
grid_search.fit(sampled_X_train, sampled_y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(best_params)

# # Train the final model with the best hyperparameters
final_model = RandomForestClassifier(**best_params, random_state=42)
final_model.fit(X_train, y_train)
print(classification_report(y_pred, y_test))

"""# Clustering Model Development

##K-means Clustering
"""

df_g = df_dropped2.groupby(['Primary Type','Location Description']).size().to_frame('count').reset_index()
df_g = df_g.pivot(index='Primary Type',columns='Location Description',values='count')
df_g.fillna(0, inplace=True)
df_g.head(10)

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_g)
# Apply PCA to reduce dimensionality
pca = PCA(n_components=30)
pca_data = pca.fit_transform(scaled_data)

# Define a range of possible cluster numbers (k)
k_range = range(1, 11)
inertia = []

# Calculate the sum of squared distances (inertia) for each k
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

# Plot the elbow method to find the optimal k
plt.figure(figsize=(8, 4))
plt.plot(k_range, inertia, marker='o', linestyle='-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k (k-means)')
plt.grid(True)
plt.show()

k = 6
kmeans = KMeans(n_clusters=k, random_state=0)
cluster_labels = kmeans.fit_predict(pca_data)

df_g['Cluster'] = cluster_labels

# Plot the data points with cluster colors
plt.figure(figsize=(8, 6))
for i in range(k):
    plt.scatter(pca_data[cluster_labels == i, 0], scaled_data[cluster_labels == i, 1], label=f'Cluster {i}')

# Plot the cluster centers
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, c='red', label='Centroids')

plt.legend()
plt.title(f'K-Means Clustering with {k} Clusters')
plt.xlabel('Primary Type')
plt.ylabel('Location Description')
plt.show()

"""##Hierarchical Clustering"""

# Perform hierarchical clustering
# Plot the dendrogram
linked = linkage(df_g, method='ward')
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.show()

"""##PAM (Partition Among Medoids)"""

# Perform PAM clustering (KMedoids) with the elbow method to determine the optimal number of clusters
k_range = range(1, 11)
inertia = []

for k in k_range:
    pam = KMedoids(n_clusters=k, random_state=0)
    pam.fit(pca_data)
    inertia.append(pam.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(k_range, inertia, marker='o', linestyle='-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k (PAM)')
plt.grid(True)
plt.show()

# Choose the optimal number of clusters
k = 4
pam = KMedoids(n_clusters=k, random_state=0)
cluster_labels_pam = pam.fit_predict(pca_data)

# Plot the clustered data
plt.figure(figsize=(8, 6))
for i in range(k):
    plt.scatter(pca_data[cluster_labels_pam == i, 0], pca_data[cluster_labels_pam == i, 1], label=f'Cluster {i}')

plt.scatter(pam.cluster_centers_[:, 0], pam.cluster_centers_[:, 1], marker='x', s=200, c='red', label='Medoids')
plt.legend()
plt.title(f'PAM Clustering with {k} Clusters')
plt.xlabel('Primary Type')
plt.ylabel('Location Description')
plt.show()